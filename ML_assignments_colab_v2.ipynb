{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786153ce",
   "metadata": {},
   "source": [
    "# ML Assignments — Polinomială, Logistică, Naive Bayes (corectat) și Regresie Liniară\n",
    "\n",
    "Notebook pregătit pentru Google Colab.\n",
    "\n",
    "Conține: \n",
    "- Regresie polinomială (fermier)\n",
    "- Regresie logistică (boală cardiacă)\n",
    "- Naive Bayes (spam) — secțiune refăcută, calcule manuale pas cu pas + Laplace\n",
    "- Regresie liniară (cafenea)\n",
    "\n",
    "Rulează celulele în Colab: Runtime → Run all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90617a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Librării\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab96d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Regresie polinomială (fermier)\n",
    "# ---------------------------\n",
    "X = np.array([10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]).reshape(-1,1)\n",
    "y = np.array([2.1,2.8,3.6,4.5,5.2,5.8,6.2,6.4,6.5,6.4,6.2,5.9,5.4,4.8,4.0])\n",
    "\n",
    "# Train = primele 12, Test = ultimele 3\n",
    "X_train, X_test = X[:12], X[12:15]\n",
    "y_train, y_test = y[:12], y[12:15]\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(X, y, color='black', label='observatii')\n",
    "xs = np.linspace(0,170,500).reshape(-1,1)\n",
    "\n",
    "results = {}\n",
    "for deg in [1,2,3,4]:\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    Xtr = poly.fit_transform(X_train)\n",
    "    Xte = poly.transform(X_test)\n",
    "    Xs = poly.transform(xs)\n",
    "    model = LinearRegression().fit(Xtr, y_train)\n",
    "    ytr_pred = model.predict(Xtr)\n",
    "    yte_pred = model.predict(Xte)\n",
    "    yxs = model.predict(Xs)\n",
    "    results[deg] = {\n",
    "        'MAE_train': mean_absolute_error(y_train, ytr_pred),\n",
    "        'MSE_train': mean_squared_error(y_train, ytr_pred),\n",
    "        'RMSE_train': np.sqrt(mean_squared_error(y_train, ytr_pred)),\n",
    "        'R2_train': r2_score(y_train, ytr_pred),\n",
    "        'MAE_test': mean_absolute_error(y_test, yte_pred),\n",
    "        'MSE_test': mean_squared_error(y_test, yte_pred),\n",
    "        'RMSE_test': np.sqrt(mean_squared_error(y_test, yte_pred)),\n",
    "        'R2_test': r2_score(y_test, yte_pred),\n",
    "        'opt_amount': float(xs[np.argmax(yxs)]),\n",
    "        'opt_value': float(np.max(yxs))\n",
    "    }\n",
    "    plt.plot(xs, yxs, label=f'deg {deg}')\n",
    "\n",
    "plt.scatter(X_train, y_train, label='train', c='blue')\n",
    "plt.scatter(X_test, y_test, label='test', c='red', marker='X', s=80)\n",
    "plt.xlabel('Fertilizator (kg/ha)'); plt.ylabel('Randament (t/ha)')\n",
    "plt.legend(); plt.title('Modele polinomiale')\n",
    "plt.show()\n",
    "\n",
    "import pprint; pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Regresie logistica (boala cardiaca)\n",
    "# ---------------------------\n",
    "age = np.array([25,30,35,40,45,50,52,55,58,60,62,65,68,70,72,75,78,80])\n",
    "chol = np.array([180,190,195,200,210,220,235,240,250,255,265,270,280,285,295,300,310,320])\n",
    "y_card = np.array([0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "X = np.vstack([age, chol]).T\n",
    "X_train, X_test = X[:14], X[14:18]\n",
    "y_train, y_test = y_card[:14], y_card[14:18]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "Xtr_s = scaler.transform(X_train)\n",
    "Xte_s = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear').fit(Xtr_s, y_train)\n",
    "ytr_pred = clf.predict(Xtr_s); yte_pred = clf.predict(Xte_s)\n",
    "ytr_prob = clf.predict_proba(Xtr_s)[:,1]; yte_prob = clf.predict_proba(Xte_s)[:,1]\n",
    "\n",
    "def print_log_metrics(y_true, y_pred, y_prob, name='set'):\n",
    "    print(f'--- Metrics for {name} ---')\n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    print('Precision:', precision_score(y_true, y_pred, zero_division=0))\n",
    "    print('Recall:', recall_score(y_true, y_pred, zero_division=0))\n",
    "    print('F1:', f1_score(y_true, y_pred, zero_division=0))\n",
    "    try:\n",
    "        print('ROC AUC:', roc_auc_score(y_true, y_prob))\n",
    "    except:\n",
    "        print('ROC AUC: N/A')\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "print_log_metrics(y_train, ytr_pred, ytr_prob, 'train')\n",
    "print_log_metrics(y_test, yte_pred, yte_prob, 'test')\n",
    "\n",
    "# predictie exemplu\n",
    "sample = np.array([[55, 260]])\n",
    "prob_sample = clf.predict_proba(scaler.transform(sample))[:,1][0]\n",
    "print('Probabilitate (55 ani, 260 mg/dL):', prob_sample)\n",
    "print('Coeficienti (age, chol):', clf.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Naive Bayes (spam) - corectat, calcule manuale pas cu pas\n",
    "# ---------------------------\n",
    "# Construim DataFrame din tabelul oferit (20 emailuri)\n",
    "data = [\n",
    "# Free, Money, Winner, Link, Spam\n",
    "[1,1,1,1,1],\n",
    "[1,1,0,1,1],\n",
    "[0,0,0,0,0],\n",
    "[1,0,1,1,1],\n",
    "[0,0,0,1,0],\n",
    "[1,1,1,1,1],\n",
    "[0,1,0,1,1],\n",
    "[0,0,0,0,0],\n",
    "[1,1,1,0,1],\n",
    "[0,0,1,0,0],\n",
    "[1,0,0,1,1],\n",
    "[0,0,0,0,0],\n",
    "[1,1,1,1,1],\n",
    "[0,1,1,1,1],\n",
    "[0,0,0,1,0],\n",
    "[1,1,0,1,1],\n",
    "[0,0,0,0,0],\n",
    "[1,0,1,1,1],\n",
    "[0,0,1,0,1],\n",
    "[0,0,0,0,0]\n",
    "]\n",
    "cols = ['Free','Money','Winner','Link','Spam']\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df.index += 1  # make index 1..20\n",
    "print('Dataset:\\n', df.head(10))  # show first rows\n",
    "\n",
    "# Split: first 16 train, last 4 test (as requested)\n",
    "df_train = df.iloc[:16].copy()\n",
    "df_test = df.iloc[16:20].copy()\n",
    "\n",
    "X_train = df_train[['Free','Money','Winner','Link']].values\n",
    "y_train = df_train['Spam'].values\n",
    "X_test = df_test[['Free','Money','Winner','Link']].values\n",
    "y_test = df_test['Spam'].values\n",
    "\n",
    "# 2) Priori (train)\n",
    "p_spam = y_train.mean()\n",
    "p_not = 1 - p_spam\n",
    "print('\\nPriors from train: P(Spam)=%.3f, P(NotSpam)=%.3f' % (p_spam, p_not))\n",
    "\n",
    "# 3) Prob conditionale (without Laplace)\n",
    "def compute_conditional_probs(X, y):\n",
    "    feats = ['Free','Money','Winner','Link']\n",
    "    cond = {}\n",
    "    for i,f in enumerate(feats):\n",
    "        # P(feature=1 | class=1) and P(feature=1 | class=0)\n",
    "        numerator_spam = np.sum(X[y==1, i]==1)\n",
    "        denom_spam = np.sum(y==1)\n",
    "        numerator_not = np.sum(X[y==0, i]==1)\n",
    "        denom_not = np.sum(y==0)\n",
    "        p1_spam = numerator_spam/denom_spam if denom_spam>0 else 0.0\n",
    "        p1_not = numerator_not/denom_not if denom_not>0 else 0.0\n",
    "        cond[f] = {'P(1|Spam)': p1_spam, 'P(1|NotSpam)': p1_not, \n",
    "                   'count_spam_with_feat': int(numerator_spam), 'count_not_with_feat': int(numerator_not)}\n",
    "    return cond\n",
    "\n",
    "cond_probs = compute_conditional_probs(X_train, y_train)\n",
    "print('\\nConditional probabilities (no Laplace):')\n",
    "for k,v in cond_probs.items():\n",
    "    print(k, v)\n",
    "\n",
    "# 4) Clasificare manuala a unui nou email: Free=1, Money=0, Winner=1, Link=1\n",
    "new = np.array([1,0,1,1])\n",
    "print('\\nNew email features:', new)\n",
    "\n",
    "def manual_nb(X_train, y_train, x, laplace=False, alpha=1.0):\n",
    "    n_spam = np.sum(y_train==1)\n",
    "    n_not = np.sum(y_train==0)\n",
    "    total = len(y_train)\n",
    "    p_spam = n_spam/total\n",
    "    p_not = n_not/total\n",
    "    feats = ['Free','Money','Winner','Link']\n",
    "    # compute conditional probs with/without Laplace\n",
    "    cond = {}\n",
    "    for i,f in enumerate(feats):\n",
    "        count_spam = np.sum(X_train[y_train==1,i]==1)\n",
    "        count_not = np.sum(X_train[y_train==0,i]==1)\n",
    "        if laplace:\n",
    "            p_f_spam = (count_spam + alpha) / (n_spam + 2*alpha)\n",
    "            p_f_not = (count_not + alpha) / (n_not + 2*alpha)\n",
    "        else:\n",
    "            p_f_spam = count_spam / n_spam if n_spam>0 else 0.0\n",
    "            p_f_not = count_not / n_not if n_not>0 else 0.0\n",
    "        cond[f] = (p_f_spam, p_f_not, int(count_spam), int(count_not))\n",
    "    # compute likelihoods (use logs to avoid underflow)\n",
    "    log_like_spam = np.log(p_spam) if p_spam>0 else -np.inf\n",
    "    log_like_not = np.log(p_not) if p_not>0 else -np.inf\n",
    "    for i,val in enumerate(x):\n",
    "        p1_spam, p1_not, _, _ = cond[feats[i]]\n",
    "        if val==1:\n",
    "            log_like_spam += np.log(p1_spam) if p1_spam>0 else -np.inf\n",
    "            log_like_not += np.log(p1_not) if p1_not>0 else -np.inf\n",
    "        else:\n",
    "            log_like_spam += np.log(1-p1_spam) if p1_spam<1 else -np.inf\n",
    "            log_like_not += np.log(1-p1_not) if p1_not<1 else -np.inf\n",
    "    maxlog = max(log_like_spam, log_like_not)\n",
    "    if np.isinf(maxlog):\n",
    "        return None, cond, (log_like_spam, log_like_not)\n",
    "    exp_spam = np.exp(log_like_spam - maxlog)\n",
    "    exp_not = np.exp(log_like_not - maxlog)\n",
    "    prob_spam = exp_spam / (exp_spam + exp_not)\n",
    "    return prob_spam, cond, (log_like_spam, log_like_not)\n",
    "\n",
    "prob_no_lap, cond_no_lap, logs_no_lap = manual_nb(X_train, y_train, new, laplace=False)\n",
    "print('\\nManual NB without Laplace:')\n",
    "print('P(Spam|x) ≈', prob_no_lap)\n",
    "print('Log-likelihoods (spam, not):', logs_no_lap)\n",
    "print('Conditional probs used (no Laplace):')\n",
    "for k,v in cond_no_lap.items():\n",
    "    print(k, 'P(1|Spam)=%.3f, P(1|Not)=%.3f, counts(spam,not)=(%d,%d)' % (v[0], v[1], v[2], v[3]))\n",
    "\n",
    "# With Laplace smoothing alpha=1\n",
    "prob_lap, cond_lap, logs_lap = manual_nb(X_train, y_train, new, laplace=True, alpha=1.0)\n",
    "print('\\nManual NB WITH Laplace (alpha=1):')\n",
    "print('P(Spam|x) ≈', prob_lap)\n",
    "print('Log-likelihoods (spam, not):', logs_lap)\n",
    "print('Conditional probs used (Laplace):')\n",
    "for k,v in cond_lap.items():\n",
    "    print(k, 'P(1|Spam)=%.3f, P(1|Not)=%.3f, counts(spam,not)=(%d,%d)' % (v[0], v[1], v[2], v[3]))\n",
    "\n",
    "# 5) Train BernoulliNB and compare\n",
    "clf = BernoulliNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('\\nBernoulliNB predict_proba for new email:', clf.predict_proba(new.reshape(1,-1))[0,1])\n",
    "print('BernoulliNB class prediction for new email:', clf.predict(new.reshape(1,-1))[0])\n",
    "\n",
    "# 6) Confusion matrices and metrics (train & test)\n",
    "for name, (Xc, yc) in [('train',(X_train,y_train)), ('test',(X_test,y_test))]:\n",
    "    pred = clf.predict(Xc)\n",
    "    print('\\n--- Metrics', name, '---')\n",
    "    print('Confusion matrix:\\n', confusion_matrix(yc, pred))\n",
    "    print('Accuracy:', accuracy_score(yc, pred))\n",
    "    print('Precision:', precision_score(yc, pred, zero_division=0))\n",
    "    print('Recall:', recall_score(yc, pred, zero_division=0))\n",
    "    print('F1:', f1_score(yc, pred, zero_division=0))\n",
    "\n",
    "# 7) Feature importance: log probability differences\n",
    "log_prob = clf.feature_log_prob_\n",
    "feat_names = ['Free','Money','Winner','Link']\n",
    "importance = {feat_names[i]: float(log_prob[1,i] - log_prob[0,i]) for i in range(len(feat_names))}\n",
    "print('\\nFeature importance (log-prob difference, higher -> more indicative of Spam):')\n",
    "for k,v in sorted(importance.items(), key=lambda x:-x[1]):\n",
    "    print(k, '%.4f' % v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ad717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Regresie liniara (cafenea)\n",
    "# ---------------------------\n",
    "X = np.array([50,60,70,80,90,100,110,120,130,140,150,160]).reshape(-1,1)\n",
    "y = np.array([200,240,280,320,360,400,440,480,520,560,600,640])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Numar clienti'); plt.ylabel('Vanzari (lei)'); plt.title('Clienți vs Vânzări')\n",
    "plt.show()\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "MAE = mean_absolute_error(y, y_pred)\n",
    "MSE = mean_squared_error(y, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "R2 = r2_score(y, y_pred)\n",
    "\n",
    "print('Coefficients: slope=', model.coef_[0], ' intercept=', model.intercept_)\n",
    "print('MAE:', MAE); print('MSE:', MSE); print('RMSE:', RMSE); print('R2:', R2)\n",
    "\n",
    "# Predict for 170 clients\n",
    "pred_170 = model.predict(np.array([[170]]))[0]\n",
    "print('\\nPredicted sales for 170 clients:', pred_170)\n",
    "print('\\nInterpretare: date strict liniare -> R2 aproape de 1. Model adecvat; predicția pentru 170 este o extindere mică (extrapolare modestă).')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
